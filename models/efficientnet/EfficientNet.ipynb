{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# EfficientNet-B0 for AI vs Real Image Detection (TensorFlow)\n",
                "\n",
                "This notebook implements the training and evaluation pipeline using EfficientNet-B0 and TensorFlow/Keras.\n",
                "**Data Loading**: Uses Hugging Face `datasets` library to load data from Parquet files.\n",
                "**Monitoring**: Includes System (CPU/RAM/Disk) and GPU monitoring.\n",
                "\n",
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install datasets pandas pyarrow psutil matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import psutil\n",
                "import threading\n",
                "import subprocess\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
                "from datasets import load_dataset\n",
                "from PIL import Image\n",
                "import io\n",
                "\n",
                "# Check for GPU\n",
                "print(\"TensorFlow version:\", tf.__version__)\n",
                "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. System Performance Monitoring\n",
                "We use a background thread to log CPU, RAM, Disk, and GPU usage during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SystemMonitor:\n",
                "    def __init__(self, interval=1.0):\n",
                "        self.interval = interval\n",
                "        self.stop_event = threading.Event()\n",
                "        self.history = {\n",
                "            'timestamp': [],\n",
                "            'cpu_percent': [],\n",
                "            'ram_percent': [],\n",
                "            'gpu_percent': [],\n",
                "            'gpu_mem': [],\n",
                "            'disk_read': [],\n",
                "            'disk_write': []\n",
                "        }\n",
                "        self.thread = threading.Thread(target=self._monitor_loop)\n",
                "\n",
                "    def _get_gpu_metrics(self):\n",
                "        try:\n",
                "            # Uses nvidia-smi to get GPU utilization and Memory usage\n",
                "            result = subprocess.check_output(\n",
                "                ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used', '--format=csv,noheader,nounits'],\n",
                "                encoding='utf-8'\n",
                "            )\n",
                "            # Output example: \"45, 1024\" -> 45% util, 1024MB mem\n",
                "            util, mem = map(int, result.strip().split(','))\n",
                "            return util, mem\n",
                "        except Exception:\n",
                "            # Fallback if nvidia-smi is missing\n",
                "            return 0, 0\n",
                "\n",
                "    def _monitor_loop(self):\n",
                "        # Initial disk counters\n",
                "        last_disk = psutil.disk_io_counters()\n",
                "        start_time = time.time()\n",
                "        \n",
                "        while not self.stop_event.is_set():\n",
                "            current_time = time.time() - start_time\n",
                "            cpu = psutil.cpu_percent(interval=None)\n",
                "            ram = psutil.virtual_memory().percent\n",
                "            gpu_util, gpu_mem = self._get_gpu_metrics()\n",
                "            \n",
                "            if last_disk:\n",
                "                current_disk = psutil.disk_io_counters()\n",
                "                disk_read = (current_disk.read_bytes - last_disk.read_bytes) / 1024 / 1024 # MB\n",
                "                disk_write = (current_disk.write_bytes - last_disk.write_bytes) / 1024 / 1024 # MB\n",
                "                last_disk = current_disk\n",
                "            else:\n",
                "                disk_read, disk_write = 0, 0\n",
                "            \n",
                "            self.history['timestamp'].append(current_time)\n",
                "            self.history['cpu_percent'].append(cpu)\n",
                "            self.history['ram_percent'].append(ram)\n",
                "            self.history['gpu_percent'].append(gpu_util)\n",
                "            self.history['gpu_mem'].append(gpu_mem)\n",
                "            self.history['disk_read'].append(disk_read)\n",
                "            self.history['disk_write'].append(disk_write)\n",
                "            \n",
                "            time.sleep(self.interval)\n",
                "\n",
                "    def start(self):\n",
                "        self.stop_event.clear()\n",
                "        self.thread = threading.Thread(target=self._monitor_loop) # Recreate thread if restarted\n",
                "        self.thread.start()\n",
                "        print(\"System monitoring started...\")\n",
                "\n",
                "    def stop(self):\n",
                "        self.stop_event.set()\n",
                "        self.thread.join()\n",
                "        print(\"System monitoring stopped.\")\n",
                "        \n",
                "    def plot(self):\n",
                "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
                "        \n",
                "        # CPU & RAM\n",
                "        ax1.plot(self.history['timestamp'], self.history['cpu_percent'], label='CPU %')\n",
                "        ax1.plot(self.history['timestamp'], self.history['ram_percent'], label='RAM %')\n",
                "        ax1.set_title('CPU & RAM Usage')\n",
                "        ax1.set_xlabel('Time (s)')\n",
                "        ax1.set_ylabel('Percentage')\n",
                "        ax1.legend()\n",
                "        ax1.grid(True)\n",
                "\n",
                "        # GPU\n",
                "        ax2.plot(self.history['timestamp'], self.history['gpu_percent'], label='GPU %', color='red')\n",
                "        ax2.set_ylabel('Utilization %', color='red')\n",
                "        ax2_mem = ax2.twinx()\n",
                "        ax2_mem.plot(self.history['timestamp'], self.history['gpu_mem'], label='GPU Mem (MB)', color='orange', linestyle='--')\n",
                "        ax2_mem.set_ylabel('Memory (MB)', color='orange')\n",
                "        ax2.set_title('GPU Usage')\n",
                "        ax2.set_xlabel('Time (s)')\n",
                "        ax2.grid(True)\n",
                "        \n",
                "        # Disk I/O\n",
                "        ax3.plot(self.history['timestamp'], self.history['disk_read'], label='Disk Read (MB)')\n",
                "        ax3.plot(self.history['timestamp'], self.history['disk_write'], label='Disk Write (MB)')\n",
                "        ax3.set_title('Disk I/O (MB per interval)')\n",
                "        ax3.set_xlabel('Time (s)')\n",
                "        ax3.set_ylabel('MB')\n",
                "        ax3.legend()\n",
                "        ax3.grid(True)\n",
                "        \n",
                "        plt.show()\n",
                "\n",
                "# Create monitor instance\n",
                "monitor = SystemMonitor(interval=1.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration & Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths (Parquet Files)\n",
                "DATA_FILES = {\n",
                "    \"train\": \"/storage/AIGeneratedImages_Midjourney/data/train-*.parquet\",\n",
                "    \"validation\": \"/storage/AIGeneratedImages_Midjourney/data/validation-*.parquet\",\n",
                "    \"test\": \"/storage/AIGeneratedImages_Midjourney/data/test-*.parquet\",\n",
                "}\n",
                "\n",
                "# Hyperparameters\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 128 # Optimized for P5000\n",
                "LEARNING_RATE = 0.001\n",
                "NUM_EPOCHS = 5 # Reduced\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Loading & Preprocessing\n",
                "Loading Parquet files and converting to TF Dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading dataset from parquet...\")\n",
                "dataset = load_dataset(\n",
                "    \"parquet\",\n",
                "    data_files=DATA_FILES\n",
                ")\n",
                "print(dataset)\n",
                "\n",
                "# Assuming the dataset has 'image' (bytes or PIL) and 'label' columns.\n",
                "# We'll assume the image column needs decoding if it's bytes, or handling if it's already a PIL object\n",
                "\n",
                "def process_example(example):\n",
                "    # Import inside function to avoid serialization issues in tf.data generator\n",
                "    from PIL import Image\n",
                "    import io\n",
                "    \n",
                "    # Check if 'image' is bytes (common in parquet) or dict (HF Image feature)\n",
                "    img_data = example['image']\n",
                "    try:\n",
                "        if isinstance(img_data, bytes):\n",
                "             image = Image.open(io.BytesIO(img_data))\n",
                "        elif isinstance(img_data, dict) and 'bytes' in img_data:\n",
                "             image = Image.open(io.BytesIO(img_data['bytes']))\n",
                "        else:\n",
                "             # Already PIL image or path?\n",
                "             image = img_data\n",
                "        \n",
                "        if not isinstance(image, Image.Image):\n",
                "            # Start simple: try opening if it's a string path\n",
                "            if isinstance(image, str):\n",
                "                 image = Image.open(image)\n",
                "    except Exception as e:\n",
                "        # Return dummy if failed (filtered out later) or raise\n",
                "        # For robustness, we generate a black image\n",
                "        image = Image.new('RGB', IMG_SIZE)\n",
                "    \n",
                "    image = image.convert(\"RGB\").resize(IMG_SIZE)\n",
                "    return np.array(image), example['label']\n",
                "\n",
                "def tf_data_generator(split_name):\n",
                "    def generator():\n",
                "        for example in dataset[split_name]:\n",
                "            yield process_example(example)\n",
                "    return generator\n",
                "\n",
                "def create_tf_dataset(split_name):\n",
                "    return tf.data.Dataset.from_generator(\n",
                "        tf_data_generator(split_name),\n",
                "        output_signature=(\n",
                "            tf.TensorSpec(shape=(IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.uint8),\n",
                "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
                "        )\n",
                "    )\n",
                "\n",
                "train_ds = create_tf_dataset('train')\n",
                "val_ds = create_tf_dataset('validation')\n",
                "test_ds = create_tf_dataset('test')\n",
                "\n",
                "# Optimization & Batching\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "\n",
                "# Ensure batching and prefetching\n",
                "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Setup: EfficientNet-B0\n",
                "Including augmentation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_augmentation = keras.Sequential(\n",
                "  [\n",
                "    layers.RandomFlip(\"horizontal\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
                "    layers.RandomRotation(0.05),\n",
                "    layers.RandomZoom(0.05),\n",
                "  ]\n",
                ")\n",
                "\n",
                "def build_model():\n",
                "    # Explicit Input Layer to assist build\n",
                "    inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
                "    x = data_augmentation(inputs)\n",
                "    \n",
                "    base_model = tf.keras.applications.EfficientNetB0(\n",
                "        include_top=False,\n",
                "        weights=\"imagenet\",\n",
                "        input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
                "    )\n",
                "    \n",
                "    base_model.trainable = False\n",
                "    \n",
                "    x = base_model(x, training=False)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dropout(0.2)(x)\n",
                "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
                "    \n",
                "    model = tf.keras.Model(inputs, outputs)\n",
                "    \n",
                "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
                "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
                "                  metrics=['accuracy', tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')])\n",
                "    return model\n",
                "\n",
                "model = build_model()\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
                "    \"efficientnet_b0_best.keras\", save_best_only=True, monitor='val_accuracy'\n",
                ")\n",
                "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
                "    patience=5, restore_best_weights=True, monitor='val_accuracy'\n",
                ")\n",
                "\n",
                "\n",
                "monitor.start()\n",
                "\n",
                "try:\n",
                "    history = model.fit(\n",
                "        train_ds,\n",
                "        validation_data=val_ds,\n",
                "        epochs=NUM_EPOCHS,\n",
                "        callbacks=[checkpoint_cb, early_stopping_cb]\n",
                "    )\n",
                "finally:\n",
                "    monitor.stop()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize System Performance\n",
                "monitor.plot()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot history\n",
                "acc = history.history['accuracy']\n",
                "val_acc = history.history['val_accuracy']\n",
                "loss = history.history['loss']\n",
                "val_loss = history.history['val_loss']\n",
                "\n",
                "epochs_range = range(len(acc))\n",
                "\n",
                "plt.figure(figsize=(8, 8))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
                "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Training and Validation Accuracy')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(epochs_range, loss, label='Training Loss')\n",
                "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Evaluating on Test Set...\")\n",
                "results = model.evaluate(test_ds)\n",
                "print(f\"Test Accuracy: {results[1]:.4f}\")\n",
                "print(f\"Test Precision: {results[2]:.4f}\")\n",
                "print(f\"Test Recall: {results[3]:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}