{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GAN Discriminator for AI vs Real Image Detection\n",
                "\n",
                "This notebook implements a **DCGAN-style Discriminator** to classify images as Real vs AI.\n",
                "It also includes system performance monitoring (CPU, RAM, Disk I/O) during training.\n",
                "\n",
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install datasets pandas pyarrow psutil matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import psutil\n",
                "import threading\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, models\n",
                "from datasets import load_dataset\n",
                "from PIL import Image\n",
                "import io\n",
                "\n",
                "# Check for GPU\n",
                "print(\"TensorFlow version:\", tf.__version__)\n",
                "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. System Performance Monitoring\n",
                "We use a background thread to log CPU, RAM, and Disk usage during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SystemMonitor:\n",
                "    def __init__(self, interval=1.0):\n",
                "        self.interval = interval\n",
                "        self.stop_event = threading.Event()\n",
                "        self.history = {\n",
                "            'timestamp': [],\n",
                "            'cpu_percent': [],\n",
                "            'ram_percent': [],\n",
                "            'disk_read': [],\n",
                "            'disk_write': []\n",
                "        }\n",
                "        self.thread = threading.Thread(target=self._monitor_loop)\n",
                "\n",
                "    def _monitor_loop(self):\n",
                "        # Initial disk counters\n",
                "        last_disk = psutil.disk_io_counters()\n",
                "        start_time = time.time()\n",
                "        \n",
                "        while not self.stop_event.is_set():\n",
                "            current_time = time.time() - start_time\n",
                "            cpu = psutil.cpu_percent(interval=None)\n",
                "            ram = psutil.virtual_memory().percent\n",
                "            \n",
                "            current_disk = psutil.disk_io_counters()\n",
                "            disk_read = (current_disk.read_bytes - last_disk.read_bytes) / 1024 / 1024 # MB\n",
                "            disk_write = (current_disk.write_bytes - last_disk.write_bytes) / 1024 / 1024 # MB\n",
                "            last_disk = current_disk\n",
                "            \n",
                "            self.history['timestamp'].append(current_time)\n",
                "            self.history['cpu_percent'].append(cpu)\n",
                "            self.history['ram_percent'].append(ram)\n",
                "            self.history['disk_read'].append(disk_read)\n",
                "            self.history['disk_write'].append(disk_write)\n",
                "            \n",
                "            time.sleep(self.interval)\n",
                "\n",
                "    def start(self):\n",
                "        self.stop_event.clear()\n",
                "        self.thread = threading.Thread(target=self._monitor_loop) # Recreate thread if restarted\n",
                "        self.thread.start()\n",
                "        print(\"System monitoring started...\")\n",
                "\n",
                "    def stop(self):\n",
                "        self.stop_event.set()\n",
                "        self.thread.join()\n",
                "        print(\"System monitoring stopped.\")\n",
                "        \n",
                "    def plot(self):\n",
                "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "        \n",
                "        # CPU & RAM\n",
                "        ax1.plot(self.history['timestamp'], self.history['cpu_percent'], label='CPU %')\n",
                "        ax1.plot(self.history['timestamp'], self.history['ram_percent'], label='RAM %')\n",
                "        ax1.set_title('CPU & RAM Usage')\n",
                "        ax1.set_xlabel('Time (s)')\n",
                "        ax1.set_ylabel('Percentage')\n",
                "        ax1.legend()\n",
                "        ax1.grid(True)\n",
                "        \n",
                "        # Disk I/O\n",
                "        ax2.plot(self.history['timestamp'], self.history['disk_read'], label='Disk Read (MB)')\n",
                "        ax2.plot(self.history['timestamp'], self.history['disk_write'], label='Disk Write (MB)')\n",
                "        ax2.set_title('Disk I/O (MB per interval)')\n",
                "        ax2.set_xlabel('Time (s)')\n",
                "        ax2.set_ylabel('MB')\n",
                "        ax2.legend()\n",
                "        ax2.grid(True)\n",
                "        \n",
                "        plt.show()\n",
                "\n",
                "# Create monitor instance\n",
                "monitor = SystemMonitor(interval=1.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths\n",
                "DATA_FILES = {\n",
                "    \"train\": \"/storage/AIGeneratedImages_Midjourney/data/train-*.parquet\",\n",
                "    \"validation\": \"/storage/AIGeneratedImages_Midjourney/data/validation-*.parquet\",\n",
                "    \"test\": \"/storage/AIGeneratedImages_Midjourney/data/test-*.parquet\",\n",
                "}\n",
                "\n",
                "# Hyperparameters\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 0.0002 # Standard GAN LR\n",
                "BETA_1 = 0.5 # Standard GAN Beta\n",
                "NUM_EPOCHS = 10\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading dataset from parquet...\")\n",
                "dataset = load_dataset(\n",
                "    \"parquet\",\n",
                "    data_files=DATA_FILES\n",
                ")\n",
                "\n",
                "def process_example(example):\n",
                "    from PIL import Image\n",
                "    import io\n",
                "    img_data = example['image']\n",
                "    try:\n",
                "        if isinstance(img_data, bytes):\n",
                "             image = Image.open(io.BytesIO(img_data))\n",
                "        elif isinstance(img_data, dict) and 'bytes' in img_data:\n",
                "             image = Image.open(io.BytesIO(img_data['bytes']))\n",
                "        else:\n",
                "             image = img_data\n",
                "        \n",
                "        if not isinstance(image, Image.Image):\n",
                "             if isinstance(image, str):\n",
                "                 image = Image.open(image)\n",
                "    except Exception as e:\n",
                "        image = Image.new('RGB', IMG_SIZE)\n",
                "    \n",
                "    image = image.convert(\"RGB\").resize(IMG_SIZE)\n",
                "    # Scale to [0, 1]\n",
                "    img_array = np.array(image) / 255.0\n",
                "    return img_array.astype(np.float32), example['label']\n",
                "\n",
                "def tf_data_generator(split_name):\n",
                "    def generator():\n",
                "        for example in dataset[split_name]:\n",
                "            yield process_example(example)\n",
                "    return generator\n",
                "\n",
                "def create_tf_dataset(split_name):\n",
                "    return tf.data.Dataset.from_generator(\n",
                "        tf_data_generator(split_name),\n",
                "        output_signature=(\n",
                "            tf.TensorSpec(shape=(IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),\n",
                "            tf.TensorSpec(shape=(), dtype=tf.int64)\n",
                "        )\n",
                "    )\n",
                "\n",
                "train_ds = create_tf_dataset('train')\n",
                "val_ds = create_tf_dataset('validation')\n",
                "test_ds = create_tf_dataset('test')\n",
                "\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
                "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Architecture: DCGAN Discriminator\n",
                "We use a standard DCGAN discriminator structure:\n",
                "- Convolutional Layers with Stride 2 (Downsampling)\n",
                "- LeakyReLU Activation\n",
                "- Dropout"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_discriminator():\n",
                "    model = models.Sequential(name=\"DCGAN_Discriminator\")\n",
                "    \n",
                "    # Input Layer (224x224x3)\n",
                "    model.add(layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))\n",
                "    \n",
                "    # Block 1\n",
                "    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same'))\n",
                "    model.add(layers.LeakyReLU(alpha=0.2))\n",
                "    model.add(layers.Dropout(0.25))\n",
                "    \n",
                "    # Block 2\n",
                "    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
                "    model.add(layers.LeakyReLU(alpha=0.2))\n",
                "    model.add(layers.Dropout(0.25))\n",
                "    \n",
                "    # Block 3\n",
                "    model.add(layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
                "    model.add(layers.LeakyReLU(alpha=0.2))\n",
                "    model.add(layers.Dropout(0.25))\n",
                "    \n",
                "    # Block 4\n",
                "    model.add(layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same'))\n",
                "    model.add(layers.LeakyReLU(alpha=0.2))\n",
                "    model.add(layers.Dropout(0.25))\n",
                "\n",
                "    # Block 5 (Optional deep block for large images)\n",
                "    model.add(layers.Conv2D(512, (3, 3), strides=(2, 2), padding='same'))\n",
                "    model.add(layers.LeakyReLU(alpha=0.2))\n",
                "    model.add(layers.Dropout(0.25))\n",
                "    \n",
                "    # Classifier Head\n",
                "    model.add(layers.GlobalAveragePooling2D())\n",
                "    model.add(layers.Dense(1, activation='sigmoid'))\n",
                "    \n",
                "    return model\n",
                "\n",
                "model = build_discriminator()\n",
                "model.summary()\n",
                "\n",
                "model.compile(\n",
                "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1),\n",
                "    loss='binary_crossentropy',\n",
                "    metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall')]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training with Monitoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom Callback to stop/start monitor (optional integration style)\n",
                "# But since we have a thread, we can just start it before fit and stop after.\n",
                "\n",
                "monitor.start()\n",
                "\n",
                "try:\n",
                "    history = model.fit(\n",
                "        train_ds,\n",
                "        validation_data=val_ds,\n",
                "        epochs=NUM_EPOCHS,\n",
                "        callbacks=[\n",
                "            keras.callbacks.ModelCheckpoint(\"gan_discriminator.keras\", save_best_only=True),\n",
                "            keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
                "        ]\n",
                "    )\n",
                "finally:\n",
                "    monitor.stop()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize System Performance\n",
                "monitor.plot()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Evaluating on Test Set...\")\n",
                "results = model.evaluate(test_ds)\n",
                "print(f\"Test Accuracy: {results[1]:.4f}\")\n",
                "print(f\"Test Precision: {results[2]:.4f}\")\n",
                "print(f\"Test Recall: {results[3]:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}